# Recurrent Neural Networks
Traditional neural networks have a major shortcoming. For example, if we want to classify what kind of event is happening at every point in a movie. It’s unclear how a traditional neural network could use its reasoning about previous events in the film to inform later ones. In these times Recurrent Neural Networks (RNN) comes to our rescue. They are networks with loops in them, allowing information to persist.

In the shown diagram, a chunk of neural network, A, looks at some input x_t and outputs a value h_t. A loop allows information to be passed from one step of the network to the next. 

## The Problem of Long-Term Dependencies
In theory, RNNs are absolutely capable of handling “long-term dependencies”. But unfortunately, as the gap grows, RNNs become unable to learn to connect the information. For example, if we are trying to predict the last word in “the clouds are in the sky,” it’s pretty obvious the next word is going to be sky. In such cases, where the gap between the relevant information and the place that it’s needed is small, RNNs can learn to use the past information.

Consider “I grew up in France… I speak fluent French”. Recent information suggests that the next word is probably the name of a language, but if we want to narrow down which language, we need the context of France, from further back. 

##Why LSTM Networks?
The long term dependency problem is that, when you have larger network through time, the gradient decays quickly during back propagation. So training a RNN having long unfolding in time becomes impossible. But LSTM avoids this decay of gradient problem by allowing you to make a super highway (cell states) through time, these highways allow the gradient to freely flow backward in time.

## LSTM Networks
All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.
In LSTMs the repeating module has four neural network layers interacting in a very special way.

## The Core Idea behind LSTMs
The horizontal line running through the top of the diagram is called the cell state, which is kind of like a conveyor belt. It’s very easy for information to just flow along it unchanged. The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates. They are composed out of a sigmoid neural net layer and a point-wise multiplication operation. The sigmoid layer outputs numbers between zero and one. A value of zero means “let nothing through,” while a value of one means “let everything through!”

The first step is to decide what information to throw away from the cell state. This decision is made by a sigmoid layer called the “forget gate layer.” 

In the blog post [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), the author discusses different experimentations using the Recurrent Neural Networks and its effectiveness. In one experiment where the author trains a character-level model, it is shown that given a huge chunk of text to RNN, it models the probability distribution of the next character in the sequence given a sequence of previous characters. This will then allow it to generate new text one character at a time which closely resembles in style to the training data. 

Similarly, experiments are done where the training data is the concatenation of Paul Graham’s essays and also learning to spell words using all of Shakespeare’s works.

It is also shown that the model is actually quite good at learning complex syntactic structures. So that author also uses the model to generate codes.

## Experimentation and Results:
We trained an LSTM network on the [Penn Tree Bank] (http://www.cis.upenn.edu/~treebank/) dataset, which is a popular benchmark for measuring quality of these models, whilst being small and relatively fast to train. The network trained was of the following configuration:
```python
num_layers = 2
num_steps = 20
hidden_size = 200
max_epoch = 4
max_max_epoch = 13
keep_prob = 1.0
lr_decay = 0.5
batch_size = 20
vocab_size = 10000
```

We then generated text starting with a blank text and appending words generated by the LSTM model till we get the desired number of sentences. We see that the generated sentences are similar in style to the training dataset.

Some example of sentences:
-	It took a must amount of short-term appears in the adult market for $ N billion in September.
-	The health decline expenses a rising N decline in tower of the N national market.
-	But the company had a closing of $ N million on N of revenue and $ N million to $ N million of its current capital.
-	As part of the `<unk>` earth `<unk>` said it was closed `<unk>` at the `<unk>` but it will have all employees to offer a few clients `<unk>` to spend about $ N million.
-	Mr. Doyle also plans to cut the debts to be surprise in the red.

where N denotes some number and `<unk>` is used in place of unusual words which are less frequently observed in the corpus.

## LSTM implementations:
- An implementation using [Theano] (http://deeplearning.net/tutorial/lstm.html).
- [Keras](https://keras.io/layers/recurrent/) implementation.
- An implementation of LSTM by [Karpathy in Torch](https://github.com/karpathy/char-rnn).
- [Tensorflow] (https://www.tensorflow.org/versions/r0.12/tutorials/recurrent/index.html) implementation.

## Some Useful Resources
- [Describing Multimedia Content using Attention-based Encoder–Decoder Networks](https://arxiv.org/pdf/1507.01053.pdf)
- [Show and Tell: image captioning open sourced in TensorFlow](https://research.googleblog.com/2016/09/show-and-tell-image-captioning-open.html)
- [Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge](https://arxiv.org/pdf/1609.06647v1.pdf)
- [Deep Visual-Semantic Alignments for Generating Image Descriptions](http://cs.stanford.edu/people/karpathy/deepimagesent/)
- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
